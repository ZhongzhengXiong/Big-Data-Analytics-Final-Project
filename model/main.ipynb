{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from hyperopt import fmin, tpe, hp, space_eval, STATUS_OK\n",
    "from functools import partial\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv', encoding='utf-8')\n",
    "train_data = shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_hour(hour):\n",
    "    if 6 <= hour and hour < 10:\n",
    "        return 0\n",
    "    if 10 <= hour and hour < 14:\n",
    "        return 1\n",
    "    if 14 <= hour and hour < 18:\n",
    "        return 2\n",
    "    if 18 <= hour and hour < 22:\n",
    "        return 3\n",
    "    if 22 <= hour or hour < 6:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data['hour_class'] = train_data['Hour'].apply(encode_hour)\n",
    "test_data['hour_class'] = test_data['Hour'].apply(encode_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc_dic = {'E': [1, 0], 'NE': [0.7, 0.7], 'N': [0, 1], 'NW': [-0.7, 0.7], \n",
    "             'W': [-1, 0], 'SW': [-0.7, -0.7], 'S': [0, -1], 'SE': [0.7, -0.7] }\n",
    "heading_coor_name = ['x', 'y']\n",
    "# encoding direction\n",
    "for data in [train_data, test_data]:\n",
    "    for heading in ['EntryHeading', 'ExitHeading']:\n",
    "        for i in [0, 1]:\n",
    "            new_col = heading + '_' + heading_coor_name[i]\n",
    "            data[new_col] = data[heading].apply(lambda x: direc_dic[x][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate “steering angle” feature\n",
    "for data in [train_data, test_data]:\n",
    "    x1 = np.array(data['EntryHeading_x'])\n",
    "    y1 = np.array(data['EntryHeading_y'])\n",
    "    x2 = np.array(data['ExitHeading_x'])\n",
    "    y2 = np.array(data['ExitHeading_y'])\n",
    "    data['steering_angle'] = np.multiply(x1, x2) + np.multiply(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"same_heading\" feautre\n",
    "# train_data['same_heading'] = \n",
    "train_data['same_heading'] = (train_data['EntryHeading'] == train_data['ExitHeading']).astype(int)\n",
    "test_data['same_heading'] = (test_data['EntryHeading'] == test_data['ExitHeading']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding city\n",
    "city_dic = {'Atlanta':0, 'Boston':1, 'Chicago':2, 'Philadelphia':3}\n",
    "for data in [train_data, test_data]:\n",
    "    data['city_code'] = data['City'].apply(lambda x: city_dic[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique id\n",
    "min_id = min(train_data['IntersectionId'].unique())\n",
    "max_id = max(train_data['IntersectionId'].unique())\n",
    "train_data['UniqueId'] = train_data['IntersectionId']\n",
    "test_data['UniqueId'] = test_data['IntersectionId']\n",
    "for i, city in enumerate(['Atlanta', 'Boston', 'Philadelphia', 'Chicago']):\n",
    "    train_city_df = train_data[train_data['City'] == city]\n",
    "    test_city_df = test_data[test_data['City'] == city]\n",
    "    train_data.loc[train_city_df.index, 'UniqueId'] = train_city_df['IntersectionId'] + i * (max_id - min_id)\n",
    "    test_data.loc[test_city_df.index, 'UniqueId'] = test_city_df['IntersectionId'] + i * (max_id - min_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle latitude and longitude data\n",
    "train_data['city_pos'] = 0\n",
    "test_data['city_pos'] = 0\n",
    "\n",
    "for city in ['Atlanta', 'Boston', 'Philadelphia', 'Chicago']:\n",
    "    train_geo_df = train_data[train_data['City'] == city]\n",
    "    test_geo_df = test_data[test_data['City'] == city]\n",
    "    kmeans = KMeans(n_clusters=10).fit(train_geo_df[['Latitude', 'Longitude']])\n",
    "    train_data.loc[train_geo_df.index, 'city_pos'] = kmeans.labels_ + city_dic[city] * 10 \n",
    "    test_geo_pred = kmeans.predict(test_data.loc[test_geo_df.index][['Latitude', 'Longitude']])\n",
    "    test_data.loc[test_geo_df.index, 'city_pos'] = test_geo_pred + city_dic[city] * 10 \n",
    "\n",
    "#  fig = plt.figure(figsize=(16, 12))\n",
    "#  colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'aqua', 'brown', 'darkblue']\n",
    "#  print color\n",
    "# for i in range(10):\n",
    "#     x = np.array(geo_df[geo_df['cluster_n'] == i]['Latitude'])\n",
    "#     y = np.array(geo_df[geo_df['cluster_n'] == i]['Longitude'])\n",
    "#     plt.scatter(x, y, c=colors[i], alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance(df):\n",
    "    df_center = pd.DataFrame({\"Atlanta\":[33.753746, -84.386330], \n",
    "                            \"Boston\":[42.361145, -71.057083], \n",
    "                                \"Chicago\":[41.881832, -87.623177], \n",
    "                                  \"Philadelphia\":[39.952583, -75.165222]})\n",
    "    df[\"CenterDistance\"] = df.apply(lambda row: math.sqrt((df_center[row.City][0] - row.Latitude) ** 2 +\n",
    "                                                              (df_center[row.City][1] - row.Longitude) ** 2) , axis=1)\n",
    "add_distance(train_data)\n",
    "add_distance(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_code_start = 0\n",
    "for city in ['Atlanta', 'Boston', 'Chicago', 'Philadelphia']:\n",
    "    \n",
    "    train_city_entry_streets = list(train_data[train_data['City'] == city]['EntryStreetName'].unique())\n",
    "    train_city_exit_streets = list(train_data[train_data['City'] == city]['ExitStreetName'].unique())\n",
    "    train_city_streets = list(set(train_city_entry_streets) | set(train_city_exit_streets))\n",
    "\n",
    "    test_city_entry_streets = list(test_data[test_data['City'] == city]['EntryStreetName'].unique())\n",
    "    test_city_exit_streets = list(test_data[test_data['City'] == city]['ExitStreetName'].unique())\n",
    "    test_city_streets = list(set(test_city_entry_streets) | set(test_city_exit_streets))\n",
    "    \n",
    "    \n",
    "    city_streets = list(set(train_city_streets) | set(test_city_streets))\n",
    "    city_streets_dic = dict(zip(city_streets, range(street_code_start,  street_code_start + len(city_streets))))\n",
    "    street_code_start += len(city_streets)\n",
    "    \n",
    "    street_cols = ['EntryStreetName', 'ExitStreetName']\n",
    "    for data in [train_data, test_data]:\n",
    "        for i, col in enumerate(['EntryStreetCode', 'ExitStreetCode']):\n",
    "            data[col] = 0\n",
    "            city_data = data[data['City'] == city]\n",
    "            data.loc[data['City'] == city, col] = city_data[street_cols[i]].apply(lambda x: city_streets_dic[x])\n",
    "    \n",
    "#     print(len(train_city_streets), len(test_city_streets))\n",
    "#     print(len(set(train_city_streets) ^ set(test_city_streets)))\n",
    "#     print(len(city_streets))\n",
    "#     print(sum(city_streets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode road type\n",
    "street_encoding = {'Street': 15, 'St': 0, 'Avenue': 1, 'Ave': 1, 'Boulevard': 2, 'Road': 3,\n",
    "                'Drive': 4, 'Lane': 5, 'Tunnel': 6, 'Highway': 7, 'Way': 8, 'Parkway': 9,\n",
    "                'Parking': 10, 'Oval': 11, 'Square': 12, 'Place': 13, 'Bridge': 14}\n",
    "def street_type(street_name, street_encoding):\n",
    "    if pd.isna(street_name):\n",
    "        return 0\n",
    "    street_name_list = street_name.split()\n",
    "    for s in street_name_list:\n",
    "        if s in street_encoding.keys():\n",
    "            return street_encoding[s]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [train_data, test_data]:\n",
    "    data['EntryStreet_type'] = data['EntryStreetName'].apply(lambda x: street_type(x, street_encoding))\n",
    "    data['ExitStreet_type'] = data['ExitStreetName'].apply(lambda x: street_type(x, street_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode path\n",
    "path_code_start = 0\n",
    "for city in ['Atlanta', 'Boston', 'Chicago', 'Philadelphia']:\n",
    "    train_paths = list(train_data.loc[train_data['City'] == city, 'Path'].unique())\n",
    "    test_paths = list(test_data.loc[test_data['City'] == city, 'Path'].unique())\n",
    "    city_paths = list(set(train_paths) | set(test_paths))\n",
    "#     print(len(train_paths), len(test_paths))\n",
    "#     print(len(list(set(train_paths) & set(test_paths))))\n",
    "#     print(len(city_paths))\n",
    "    city_paths_dic = dict(zip(city_paths, range(path_code_start,  path_code_start + len(city_paths))))\n",
    "    path_code_start += len(city_paths)\n",
    "    \n",
    "    for data in [train_data, test_data]:\n",
    "        data['path_code'] = 0\n",
    "        city_data = data[data['City'] == city]\n",
    "        data.loc[data['City'] == city, 'path_code'] = city_data['Path'].apply(lambda x: city_paths_dic[x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Latitude', 'Longitude', 'EntryHeading_x', 'EntryHeading_y',\n",
    "                'ExitHeading_x', 'ExitHeading_y', 'Hour', 'Weekend', \n",
    "                'Month', 'UniqueId', 'CenterDistance', 'city_pos',\n",
    "                'EntryStreetCode', 'ExitStreetCode', 'hour_class', 'EntryStreet_type',\n",
    "                'ExitStreet_type', 'steering_angle', 'same_heading']\n",
    "target_cols = [\n",
    "    'TotalTimeStopped_p20',\n",
    "    'TotalTimeStopped_p50',\n",
    "    'TotalTimeStopped_p80',\n",
    "    'DistanceToFirstStop_p20',\n",
    "    'DistanceToFirstStop_p50',\n",
    "    'DistanceToFirstStop_p80',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['UniqueId', 'Hour', 'Weekend', 'Month', \n",
    "                        'EntryStreetCode', 'ExitStreetCode', 'city_pos', 'hour_class',\n",
    "                        'EntryStreet_type', 'ExitStreet_type', 'same_heading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(train_data) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.iloc[:train_size][feature_cols]\n",
    "y_train = train_data.iloc[:train_size][target_cols]\n",
    "X_val = train_data.iloc[train_size:][feature_cols]\n",
    "y_val = train_data.iloc[train_size:][target_cols]\n",
    "X_test = test_data[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(params, lgb_train, lgb_val, model_name='', boost_round=2000, early_stop=20, verbose_eval=200):\n",
    "    print('Starting training...')\n",
    "    gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=boost_round,\n",
    "                valid_sets=lgb_val,\n",
    "                early_stopping_rounds=early_stop, verbose_eval=200)\n",
    "\n",
    "    print('Saving model...')\n",
    "    # save model to file\n",
    "    if model_name != '':\n",
    "        gbm.save_model('{}.txt'.format(model_name))\n",
    "    # eval\n",
    "#     print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)\n",
    "    return gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train['TotalTimeStopped_p20'], \n",
    "                        categorical_feature=categorical_features, free_raw_data=False)\n",
    "lgb_val = lgb.Dataset(X_val, y_val['TotalTimeStopped_p20'], reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'rmse'},\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(params, lgb_train, lgb_val, X_val, y_val):\n",
    "    gbm = lgb_model(params, lgb_train, lgb_val)\n",
    "    y_pred = gbm.predict(X_val, num_iteration=gbm.best_iteration)\n",
    "    rmse_val = mean_squared_error(y_val, y_pred) ** 0.5\n",
    "    return {'loss': rmse_val ,  'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using hyperopt to tune parameters of LightGBM\n",
    "def hyperopt_lgb(lgb_objective, lgb_train, lgb_val, X_val, y_val):\n",
    "    space = {'objective': 'regression',\n",
    "             'metric':'rmse',\n",
    "             'boosting':'gbdt',\n",
    "             'num_leaves': hp.choice('num_leaves', list(range(20, 800, 20))),\n",
    "             'feature_fraction': hp.choice('feature_fraction', [.7, .8, .9, 1]),\n",
    "             'bagging_fraction': hp.uniform('bagging_fraction', 0.7, 1),\n",
    "             'learning_rate': hp.uniform('learning_rate', 0.03, 0.12),\n",
    "            }\n",
    "    fmin_objective = partial(lgb_objective,lgb_train=lgb_train, lgb_val=lgb_val, X_val=X_val, y_val=y_val)\n",
    "    best_vals = fmin(fmin_objective, space, algo=tpe.suggest, max_evals=10)\n",
    "    best_params = space_eval(space, best_vals)\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune lightGBM parameters\n",
    "for i, target in enumerate(target_cols):\n",
    "    print(\"Target: {}\".format(target))\n",
    "    lgb_train = lgb.Dataset(X_train, y_train[target], \n",
    "                        categorical_feature=categorical_features, free_raw_data=False)\n",
    "    lgb_val = lgb.Dataset(X_val, y_val[target], reference=lgb_train)\n",
    "    best_params = hyperopt_lgb(lgb_objective, lgb_train, lgb_val, X_val, y_val[target])\n",
    "    file_name='{}_lgb_params_19features_new2.txt'.format(target)\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(json.dumps(best_params)) \n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submissions = []\n",
    "models = []\n",
    "for i, target in enumerate(target_cols):\n",
    "    print(\"Target: {}\".format(target))\n",
    "    lgb_train = lgb.Dataset(X_train, y_train[target], \n",
    "                        categorical_feature=categorical_features, free_raw_data=False)\n",
    "    lgb_val = lgb.Dataset(X_val, y_val[target], reference=lgb_train)\n",
    "    f = open('{}_lgb_params_19features_new.txt'.format(target), 'r')\n",
    "    dic_str = f.readline()\n",
    "    params = json.loads(dic_str)\n",
    "    f.close()\n",
    "    gbm = lgb_model(params, lgb_train, lgb_val, '{}_19features_new'.format(target), boost_round=2000)\n",
    "    models.append(gbm)\n",
    "    print('Starting predicting...')\n",
    "    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "    sub_target_id = list(test_data['RowId'].apply(lambda x: '{}_{}'.format(x, i)).values)\n",
    "    sub_target_df = pd.DataFrame({'TargetId': sub_target_id, 'Target': y_pred})\n",
    "    submissions.append(sub_target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(submissions).to_csv('submission_lgb_19features_fine_tuned_new2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train_features.csv', index=False)\n",
    "test_data.to_csv('test_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check feature importance of light GBM\n",
    "models = []\n",
    "feature_importances = []\n",
    "for target in target_cols:\n",
    "    model = lgb.Booster(model_file='{}_19features_new.txt'.format(target))\n",
    "    models.append(model)\n",
    "    tmp_df = pd.DataFrame(\n",
    "        { 'column': model.feature_name(), \n",
    "         'importance': model.feature_importance(importance_type='gain'), \n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "    feature_importances.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(feature_importances):\n",
    "    df.to_csv('{}_12100811_feature_importance.csv'.format(target_cols[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_importance(models[0], tree_index=1, figsize=(20, 20), show_info=['split_gain'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xgboost model\n",
    "xgb_params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'reg:squarederror',  # 回归\n",
    "    'gamma': 0.1,                  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。\n",
    "    'max_depth': 10,               # 构建树的深度，越大越容易过拟合\n",
    "    'lambda': 1.,                  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    'subsample': 0.8,              # 随机采样训练样本\n",
    "    'colsample_bytree': 0.7,       # 生成树时进行的列采样\n",
    "    'min_child_weight': 3,\n",
    "    'silent': 1,                   # 设置成1则没有运行信息输出，最好是设置为0.\n",
    "    'eta': 0.05,                   # 如同学习率\n",
    "    'seed': 1000,\n",
    "    'nthread': 2,                  # cpu 线程数\n",
    "    'eval_metric': 'rmse',         # 评价指标\n",
    "    'min_child_weight': 20\n",
    "} \n",
    "\n",
    "nround = 10\n",
    "xgb_submissions = []\n",
    "for i, target in enumerate(target_cols):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    xgb_train = xgb.DMatrix(X_train, label=y_train[target])\n",
    "    xgb_valid = xgb.DMatrix(X_val, label=y_val[target])\n",
    "\n",
    "    watchlist = [(xgb_valid, 'valid')]\n",
    "    xgb_model = xgb.train(xgb_params, xgb_train, nround, evals=watchlist,\n",
    "                      verbose_eval=1, early_stopping_rounds=50)\n",
    "    print(\"Best Iteration: {}\".format(xgb_model.best_iteration))\n",
    "#     xgb_model.save_model('{}_xgb.model'.format(target))\n",
    "#     pv = xgb_model.predict(xgb_valid,)\n",
    "#     mse = np.mean((pv - y_val[target]) ** 2)\n",
    "#     print(target, 'rmse', np.sqrt(mse))\n",
    "    # prediction\n",
    "    y_pred = xgb_model.predict(xgb_test, ntree_limit=xgb_model.best_iteration)\n",
    "    xgb_sub_target_id = list(test_data['RowId'].apply(lambda x: '{}_{}'.format(x, i)).values)\n",
    "    xgb_sub_target_df = pd.DataFrame({'TargetId': xgb_sub_target_id, 'Target': y_pred})\n",
    "    xgb_submissions.append(xgb_sub_target_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = []\n",
    "xgb_test = xgb.DMatrix(X_test)\n",
    "for i, target in enumerate(target_cols):\n",
    "    xgb_model = xgb.Booster({'nthread': 2})  # init model\n",
    "    xgb_model.load_model('{}_xgb.model'.format(target)) \n",
    "#     xgb_model = xgb.Booster(model_file='{}_xgb.model'.format())\n",
    "    y_pred = xgb_model.predict(xgb_test)\n",
    "    sub_target_id = list(test_data['RowId'].apply(lambda x: '{}_{}'.format(x, i)).values)\n",
    "    sub_target_df = pd.DataFrame({'TargetId': sub_target_id, 'Target': y_pred})\n",
    "    submissions.append(sub_target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(submissions).to_csv('submission_xgb_19features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_features_indices = np.where(X_train.dtypes != np.float)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {\n",
    "    'iterations': 500,\n",
    "    'learning_rate': 0.01,\n",
    "    'eval_metric': 'RMSE',\n",
    "    'random_seed': 42,\n",
    "    'logging_level': 'Silent',\n",
    "    'use_best_model': True,\n",
    "    'od_type': 'Iter',\n",
    "    'od_wait': 40,\n",
    "    'task_type': \"GPU\",\n",
    "    'devices': '1:3',\n",
    "    'loss_function':'RMSE',\n",
    "    'grow_policy': 'Lossguide',\n",
    "    'logging_level': 'Verbose'\n",
    "}\n",
    "\n",
    "cat_submissions = []\n",
    "test_pool = Pool(X_test, cat_features=categorical_features_indices)\n",
    "for i, target in enumerate(target_cols):\n",
    "    train_pool = Pool(X_train, y_train[target], categorical_features_indices)\n",
    "    val_pool = Pool(X_val, y_val[target], categorical_features_indices)\n",
    "    # specify the training parameters \n",
    "    cat_model = CatBoostRegressor(**cat_params,)\n",
    "    #train the model\n",
    "    cat_model.fit(train_pool, eval_set=val_pool)\n",
    "    # make the prediction using the resulting model\n",
    "    preds = model.predict(test_pool)\n",
    "    cat_sub_target_id = list(test_data['RowId'].apply(lambda x: '{}_{}'.format(x, i)).values)\n",
    "    cat_sub_target_df = pd.DataFrame({'TargetId': cat_sub_target_id, 'Target': y_pred})\n",
    "    cat_submissions.append(cat_sub_target_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
